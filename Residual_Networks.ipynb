{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zUDTCXtIiwWP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from resnets_utils import *\n",
    "from tensorflow.keras.initializers import RandomUniform, glorot_uniform, constant, identity\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4yQbqoiiwWT"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - The Problem of Very Deep Neural Networks\n",
    "\n",
    "\n",
    "* The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the shallower layers, closer to the input) to very complex features (at the deeper layers, closer to the output). \n",
    "\n",
    "* However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent prohibitively slow.\n",
    "\n",
    "* More specifically, during gradient descent, as you backpropagate from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and \"explode,\" from gaining very large values). \n",
    "\n",
    "* During training, you might therefore see the magnitude (or norm) of the gradient for the shallower layers decrease to zero very rapidly as training proceeds, as shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LDCYE7siwWU"
   },
   "source": [
    "<img src=\"images/vanishing_grad_kiank.png\" style=\"width:450px;height:220px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 1</b> </u><font color='purple'>  : <b>Vanishing gradient</b> <br> The speed of learning decreases very rapidly for the shallower layers as the network trains </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JojZXmk-iwWV"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Building a Residual Network\n",
    "\n",
    "In ResNets, a \"shortcut\" or a \"skip connection\" allows the model to skip layers:  \n",
    "\n",
    "<img src=\"images/skip_connection_kiank.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 2</b> </u><font color='purple'>  : A ResNet block showing a skip-connection <br> </center></caption>\n",
    "\n",
    "The image on the left shows the \"main path\" through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network. \n",
    "\n",
    "Having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function. This means that you can stack on additional ResNet blocks with little risk of harming training set performance.  \n",
    "    \n",
    "On that note, there is also some evidence that the ease of learning an identity function accounts for ResNets' remarkable performance even more than skip connections help with vanishing gradients.\n",
    "\n",
    "Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are the same or different: the \"identity block\" and the \"convolutional block.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqqCnKsJiwWV"
   },
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - The Identity Block\n",
    "\n",
    "The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$). To flesh out the different steps of what happens in a ResNet's identity block, here is an alternative diagram showing the individual steps:\n",
    "\n",
    "<img src=\"images/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 3</b> </u><font color='purple'>  : <b>Identity block.</b> Skip connection \"skips over\" 2 layers. </center></caption>\n",
    "\n",
    "The upper path is the \"shortcut path.\" The lower path is the \"main path.\" In this diagram, notice the CONV2D and ReLU steps in each layer. To speed up training, a BatchNorm step has been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcL51FnRiwWW"
   },
   "source": [
    "<a name='step-1'></a>\n",
    "### Identity_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lFbVcJ-5iwWW"
   },
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, training=True, initializer=RandomUniform):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    training -- True: Behave in training mode\n",
    "                False: Behave in inference mode\n",
    "    initializer -- to set up the initial weights of a layer. Equals to random uniform initializer\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    cache = []\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size=(f, f), strides = (1, 1), padding='same', kernel_initializer = initializer(seed = 0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training = training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size=(1, 1), strides = (1, 1), padding='valid', kernel_initializer = initializer(seed = 0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training = training)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X_shortcut, X])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8igjTS2xiwWX"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - The Convolutional Block\n",
    "\n",
    "The ResNet \"convolutional block\" is the second block type. You can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path: \n",
    "\n",
    "<img src=\"images/convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 4</b> </u><font color='purple'>  : <b>Convolutional block</b> </center></caption>\n",
    "\n",
    "* The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix $W_s$ discussed in lecture.) \n",
    "* For example, to reduce the activation dimensions's height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. \n",
    "* The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B92NGqkRiwWY"
   },
   "source": [
    "<a name='step-2'></a>    \n",
    "### Convolutional_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vWcsvpg-iwWY"
   },
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, s = 2, training=True, initializer=glorot_uniform):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    training -- True: Behave in training mode\n",
    "                False: Behave in inference mode\n",
    "    initializer -- to set up the initial weights of a layer. Equals to Glorot uniform initializer, \n",
    "                   also called Xavier uniform initializer.\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of main path\n",
    "    X = Conv2D(F2, (f, f), strides = (1, 1), padding = 'same', kernel_initializer= initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training = training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(F3, (1, 1), strides = (1, 1), kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training = training)\n",
    "    \n",
    "    ##### SHORTCUT PATH #####\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides = (s, s), kernel_initializer=initializer(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3)(X_shortcut, training = training)\n",
    "\n",
    "    # Final step: Add shortcut value to main path and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS6v0EuiiwWZ"
   },
   "source": [
    "<a name='3'></a>  \n",
    "## 3 - Building ResNet Model (50 layers)\n",
    "\n",
    "We now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means we should stack 3 identity blocks together.\n",
    "\n",
    "<img src=\"images/resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 5</b> </u><font color='purple'>  : <b>ResNet-50 model</b> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XlmCd6qAiwWZ"
   },
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the popular ResNet50:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "  \n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "\n",
    "    # Stage 4\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "\n",
    "    # Stage 5 \n",
    "    X = convolutional_block(X, f = 3, filters=[512, 512, 2048], s = 2)\n",
    "    X = identity_block(X, f = 3, filters = [512, 512, 2048])\n",
    "    X = identity_block(X, f = 3, filters = [512, 512, 2048])\n",
    "\n",
    "    # AVGPOOL\n",
    "    X = AveragePooling2D(pool_size=(2, 2), name = 'avg_pool')(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlfxI0OYiwWa",
    "outputId": "2e3cd2f6-4016-4dcf-bd22-8aa5573bf023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 70, 70, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 64)   9472        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 64)   256         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 32, 32, 64)   0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 64)   0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 15, 15, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 15, 15, 64)   256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 15, 15, 64)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 15, 15, 64)   36928       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 15, 15, 64)   256         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 15, 15, 64)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 15, 15, 256)  16640       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 15, 15, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 15, 15, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 15, 15, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 15, 15, 256)  0           batch_normalization_56[0][0]     \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 15, 15, 256)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 15, 15, 64)   16448       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 15, 15, 64)   256         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 15, 15, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 15, 15, 64)   36928       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 15, 15, 64)   256         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 15, 15, 64)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 15, 15, 256)  16640       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 15, 15, 256)  1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 15, 15, 256)  0           activation_52[0][0]              \n",
      "                                                                 batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 15, 15, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 15, 15, 64)   16448       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 15, 15, 64)   256         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 15, 15, 64)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 15, 15, 64)   36928       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 15, 15, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 15, 15, 64)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 15, 15, 256)  16640       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 15, 15, 256)  1024        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 15, 15, 256)  0           activation_55[0][0]              \n",
      "                                                                 batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 15, 15, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 8, 8, 128)    32896       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 8, 128)    512         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 8, 8, 128)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 8, 8, 128)    147584      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 8, 128)    512         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 8, 8, 128)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 8, 8, 512)    66048       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 8, 8, 512)    131584      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 8, 512)    2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 8, 512)    2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 512)    0           batch_normalization_66[0][0]     \n",
      "                                                                 batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 8, 512)    0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 8, 128)    65664       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 128)    512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 8, 128)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 8, 128)    147584      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 128)    512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 8, 128)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 512)    66048       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 512)    2048        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 8, 8, 512)    0           activation_61[0][0]              \n",
      "                                                                 batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 8, 8, 512)    0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 128)    65664       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 128)    512         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 8, 8, 128)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 128)    147584      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 128)    512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 8, 128)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 512)    66048       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 512)    2048        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 8, 8, 512)    0           activation_64[0][0]              \n",
      "                                                                 batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 8, 512)    0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 8, 128)    65664       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 128)    512         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 8, 128)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 8, 128)    147584      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 128)    512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 8, 128)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 512)    66048       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 512)    2048        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 8, 8, 512)    0           activation_67[0][0]              \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 8, 512)    0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 4, 4, 256)    131328      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 4, 256)    1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 4, 4, 256)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 4, 4, 256)    590080      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 4, 256)    1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 4, 4, 256)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 4, 4, 1024)   263168      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 4, 4, 1024)   525312      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 4, 1024)   4096        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 4, 1024)   4096        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_79[0][0]     \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 4, 4, 1024)   0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 4, 256)    262400      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 4, 256)    1024        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4, 4, 256)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 4, 256)    590080      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 4, 256)    1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 4, 4, 256)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 4, 1024)   263168      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 4, 1024)   4096        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 4, 4, 1024)   0           activation_73[0][0]              \n",
      "                                                                 batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 4, 1024)   0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 4, 256)    262400      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 4, 256)    1024        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 4, 256)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 4, 256)    590080      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 4, 256)    1024        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 4, 256)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 4, 1024)   263168      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 4, 1024)   4096        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 4, 4, 1024)   0           activation_76[0][0]              \n",
      "                                                                 batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 4, 1024)   0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 4, 256)    262400      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 4, 256)    1024        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 4, 256)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 4, 256)    590080      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 4, 256)    1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 4, 256)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 4, 1024)   263168      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 4, 1024)   4096        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 4, 4, 1024)   0           activation_79[0][0]              \n",
      "                                                                 batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 4, 1024)   0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 4, 256)    262400      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 4, 256)    1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 4, 256)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 4, 256)    590080      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 4, 256)    1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 4, 256)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 4, 1024)   263168      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 4, 1024)   4096        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 4, 4, 1024)   0           activation_82[0][0]              \n",
      "                                                                 batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 4, 1024)   0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 4, 256)    262400      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 4, 4, 256)    1024        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 4, 256)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 4, 4, 256)    590080      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 4, 4, 256)    1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 4, 256)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 4, 4, 1024)   263168      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 4, 4, 1024)   4096        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 4, 4, 1024)   0           activation_85[0][0]              \n",
      "                                                                 batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 4, 1024)   0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 2, 2, 512)    524800      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 2, 2, 512)    2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 2, 2, 512)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 2, 2, 512)    2359808     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 2, 2, 512)    2048        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 2, 2, 512)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 2, 2, 2048)   2099200     activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2, 2, 2048)   8192        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 2, 2, 2048)   8192        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 2, 2, 2048)   0           batch_normalization_98[0][0]     \n",
      "                                                                 batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 2, 2, 2048)   0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 2, 512)    1049088     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2, 2, 512)    2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 2, 2, 512)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 2, 512)    2359808     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 2, 2, 512)    2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 2, 512)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 2, 2, 2048)   8192        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 2, 2, 2048)   0           activation_91[0][0]              \n",
      "                                                                 batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 2, 2048)   0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 2, 512)    1049088     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 2, 2, 512)    2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 2, 512)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 2, 512)    2359808     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 2, 2, 512)    2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 2, 512)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 2, 2, 2048)   8192        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 2, 2, 2048)   0           activation_94[0][0]              \n",
      "                                                                 batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 2, 2048)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            12294       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,600,006\n",
      "Trainable params: 23,546,886\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IVMGo2ZMiwWb"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrMB-iTIiwWb"
   },
   "source": [
    "Let's load the SIGNS dataset.\n",
    "\n",
    "<img src=\"images/signs_data_kiank.png\" style=\"width:450px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 6</b> </u><font color='purple'>  : <b>SIGNS dataset</b> </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvnkEGy6iwWc",
    "outputId": "bdfabd9b-443f-4d42-8021-65f5fbd0b331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig / 255.\n",
    "X_test = X_test_orig / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "id": "8kd5XGewiwWc",
    "outputId": "c8750685-df11-4e76-80f3-d1029972f810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "34/34 [==============================] - 11s 135ms/step - loss: 0.9207 - accuracy: 0.8078\n",
      "Epoch 2/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.3307 - accuracy: 0.8752\n",
      "Epoch 3/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.3290 - accuracy: 0.8996\n",
      "Epoch 4/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.4708 - accuracy: 0.9002\n",
      "Epoch 5/21\n",
      "34/34 [==============================] - 4s 130ms/step - loss: 0.1999 - accuracy: 0.9375\n",
      "Epoch 6/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.1308 - accuracy: 0.9584\n",
      "Epoch 7/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0687 - accuracy: 0.9717\n",
      "Epoch 8/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0765 - accuracy: 0.9712\n",
      "Epoch 9/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.2018 - accuracy: 0.9368\n",
      "Epoch 10/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.1682 - accuracy: 0.9506\n",
      "Epoch 11/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0527 - accuracy: 0.9830\n",
      "Epoch 12/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0231 - accuracy: 0.9944\n",
      "Epoch 13/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0571 - accuracy: 0.9800\n",
      "Epoch 14/21\n",
      "34/34 [==============================] - 4s 132ms/step - loss: 0.0350 - accuracy: 0.9925\n",
      "Epoch 15/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0198 - accuracy: 0.9921\n",
      "Epoch 16/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 17/21\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 18/21\n",
      " 5/34 [===>..........................] - ETA: 3s - loss: 2.1128e-04 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c27ca7602c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 21, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecFTio1HiwWc",
    "outputId": "47fde6c9-e8ee-48d7-ddfe-1ac6f5e3051a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 61ms/step - loss: 0.1709 - accuracy: 0.9667\n",
      "Loss = 0.17088894546031952\n",
      "Test Accuracy = 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L53t-aJiwWd"
   },
   "source": [
    "<a name='4'></a>  \n",
    "## 4 - Test on Your Own Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlYEWwBHiwWd"
   },
   "outputs": [],
   "source": [
    "img_path = 'images/my_image.jpg'\n",
    "img = image.load_img(img_path, target_size=(64, 64))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x/255.0\n",
    "print('Input image shape:', x.shape)\n",
    "imshow(img)\n",
    "prediction = pre_trained_model.predict(x)\n",
    "print(\"Class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] = \", prediction)\n",
    "print(\"Class:\", np.argmax(prediction))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Residual_Networks2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
